LLMManager

A Swift-based manager for downloading and running a local LLM (TinyLlama-1.1B-Chat-v1.0-GGUF) using the SpeziLLM framework on iOS.

Current Issue

During response generation, the model file is reported as missing or inaccessible, despite a successful download. Below is a key excerpt from the debug log:

SpeziLLMLocal: Local LLM file could not be opened, indicating that the model file doesn't exist
âŒ Response Generation Error: LLM file not found.

Debug Log Excerpt

...
ğŸš€ Model download started
...
âœ… Download completed, temporary path: /private/var/mobile/Containers/Data/...
âœ… File moved successfully: /var/mobile/Containers/Data/...
ğŸ“Š Saved model file size: 637699456 bytes
...
SpeziLLMLocal: Local LLM file could not be opened, indicating that the model file doesn't exist
âŒ Response Generation Error: LLM file not found.



Debuging

LLMManager initialized
Loading module(s) LLMRunner ...
Loading module(s) LLMManager ...
Model storage path: /var/mobile/Containers/Data/Application/DB697490-0400-476B-8E57-EBD26B7DA1A8/Documents/LLMModels
Attempt to present <UIAlertController: 0x1026a4c00> on <_TtGC7SwiftUI19UIHostingControllerGVS_15ModifiedContentVS_7AnyViewVS_12RootModifier__: 0x101939340> (from <_TtGC7SwiftUI19UIHostingControllerGVS_15ModifiedContentVS_7AnyViewVS_12RootModifier__: 0x101939340>) which is already presenting <UIAlertController: 0x1026a5200>.

=== Starting model download ===
ğŸ“¥ Download URL: https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_0.gguf
ğŸ“‚ Download directory: /var/mobile/Containers/Data/Application/DB697490-0400-476B-8E57-EBD26B7DA1A8/Documents/LLMDownloads
âœ… Download directory created
ğŸ“„ Model file save path: /var/mobile/Containers/Data/Application/DB697490-0400-476B-8E57-EBD26B7DA1A8/Documents/LLMDownloads/tinyllama-1.1b-chat-v1.0.Q4_0
ğŸš€ Model download started - updating UI
âœ… Download task started
â³ Download progress: 10%
â³ Download progress: 20%
â³ Download progress: 30%
â³ Download progress: 40%
â³ Download progress: 50%
â³ Download progress: 60%
â³ Download progress: 70%
â³ Download progress: 80%
â³ Download progress: 90%
âœ… Download response code: 200
ğŸ“Š Content size: 637699456 bytes
â³ Download progress: 100%
âœ… Download complete, temporary path: /private/var/mobile/Containers/Data/Application/DB697490-0400-476B-8E57-EBD26B7DA1A8/tmp/CFNetworkDownload_IuRSUD.tmp
âœ… File move complete: /var/mobile/Containers/Data/Application/DB697490-0400-476B-8E57-EBD26B7DA1A8/Documents/LLMDownloads/tinyllama-1.1b-chat-v1.0.Q4_0
ğŸ“Š Saved model file size: 637699456 bytes
ğŸ”„ Attempting to load the downloaded model
Model storage path: /var/mobile/Containers/Data/Application/DB697490-0400-476B-8E57-EBD26B7DA1A8/Documents/LLMModels
ğŸ“‚ Downloaded model path: /var/mobile/Containers/Data/Application/DB697490-0400-476B-8E57-EBD26B7DA1A8/Documents/LLMDownloads/tinyllama-1.1b-chat-v1.0.Q4_0
âœ… Schema details: LLMLocalSchema(parameters: SpeziLLMLocal.LLMLocalParameters(systemPrompt: Optional("You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe and still concise. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information."), maxOutputLength: 512, extraEOSTokens: Set([]), displayEveryNTokens: 4, seed: nil, chatTemplate: nil), samplingParameters: SpeziLLMLocal.LLMLocalSamplingParameters(topP: 1.0, temperature: 0.6, penaltyRepeat: nil, repetitionContextSize: 20), injectIntoContext: false, configuration: MLXLMCommon.ModelConfiguration(id: MLXLMCommon.ModelConfiguration.Identifier.id("/var/mobile/Containers/Data/Application/DB697490-0400-476B-8E57-EBD26B7DA1A8/Documents/LLMDownloads/tinyllama-1.1b-chat-v1.0.Q4_0"), tokenizerId: nil, overrideTokenizer: nil, defaultPrompt: "hello", extraEOSTokens: Set([])))
ğŸ“ Generated session info: Optional(SpeziLLMLocal.LLMLocalSession)
ğŸš€ Model load complete! modelLoaded = true
Can't find or decode reasons
Failed to get or decode unavailable reasons
Can't find or decode disabled use cases
<0x10253d680> Gesture: System gesture gate timed out.

=== Starting response generation ===
ğŸ“‚ Currently selected model: tinyllama-1.1b-chat-v1.0.Q4_0.gguf
âœ… Checking for existing session
ğŸ“ Session info: SpeziLLMLocal.LLMLocalSession
ğŸ’¬ Prompt to be generated: USER: Hi

ASSISTANT: 
âœ… Prompt successfully added to context
ğŸ”„ Starting token generation...
SpeziLLMLocal: Local LLM is being initialized
SpeziLLMLocal: Local LLM file could not be opened, indicating that the model file doesn't exist
âŒ Response generation error: LLM file not found.
